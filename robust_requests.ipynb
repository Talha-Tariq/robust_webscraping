{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import requests, bs4\n",
    "import pandas as pd\n",
    "import proxyscrape\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from proxyscrape import create_collector, get_collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy Scraping Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_proxies(collector, timeout_filtering, url_testing):\n",
    "    \n",
    "    #Refresh list of proxies to scrape \n",
    "    collector.refresh_proxies(force=True)\n",
    "    \n",
    "    #Scrape HTTPS proxies that provide anonymity for specific countries. See proxyscrape documentation for more info.\n",
    "#     proxy_list = collector.get_proxy({'code': ('us'), 'anonymous': True, 'type': 'https'}) #Get one USA proxy\n",
    "#     proxy_list = collector.get_proxies({'code': ('us'), 'anonymous': True, 'type': 'https'}) #Get ALL USA proxies\n",
    "    proxy_list = collector.get_proxies({'code': ('us', 'ca'), 'anonymous': True, 'type': 'https'}) #Get ALL USA + Canadian proxies\n",
    "#     proxy_list = collector.get_proxies({'code': ('us', 'ca', 'de', 'fr'), 'anonymous': True, 'type': 'https'}) ALL USA+Canada+German+French\n",
    "    \n",
    "    #Let's extract useful information from this scraped list of proxies\n",
    "    #Note that scraped list may contain duplicates\n",
    "    \n",
    "    ip_addresses = []\n",
    "    ports = []\n",
    "    countries = []\n",
    "\n",
    "    #Collecting proxy information\n",
    "    for proxy in proxy_list:\n",
    "        if proxy[0] in ip_addresses and proxy[1] in ports: #preventing duplicates from being recorded\n",
    "            continue\n",
    "\n",
    "        ip_addresses.append(proxy[0]) #Collect the ip addresses\n",
    "        ports.append(proxy[1]) #Collect the port numbers\n",
    "        countries.append(proxy[3]) #Collect their corresponding countries\n",
    "    \n",
    "    #Now let's filter out the working (good) proxies\n",
    "    \n",
    "    good_proxies = [] \n",
    "    good_proxies_country = []\n",
    "\n",
    "    for ip_address, port, country in zip(ip_addresses, ports, countries):\n",
    "        full_proxy = ip_address + \":\" + port\n",
    "\n",
    "        proxy_temp = {\"https\": \"https://\" + full_proxy} #putting it in correct format\n",
    "        print(\"Trying Proxy: \" + \"https://\" + full_proxy)\n",
    "\n",
    "        #Filtering proxies\n",
    "        try:\n",
    "            requests.get(url_testing, proxies=proxy_temp, timeout=timeout_filtering) #Testing proxy by downloading content in URL with a timeout \n",
    "            print(\"Success! Added to List.\") #Successful downloading means that the proxy works\n",
    "            good_proxies.append(full_proxy)\n",
    "            good_proxies_country.append(country)\n",
    "        except:\n",
    "            print(\"Connection error. Trying next proxy.\")\n",
    "    \n",
    "    return good_proxies, good_proxies_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Proxy Collection here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying Proxy: https://3.135.90.78:17567\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://166.70.207.2:8118\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://107.178.4.215:35330\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://104.45.188.43:3128\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://50.192.195.69:52018\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://54.214.52.181:80\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://24.172.34.114:49920\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://104.236.48.178:8080\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://198.204.253.114:3128\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://50.233.228.147:8080\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://144.217.101.245:3129\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://34.105.59.26:80\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://162.17.252.5:43764\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://160.2.38.41:8080\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://192.3.31.67:9998\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://173.82.177.226:5836\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://209.190.32.28:3128\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://198.55.103.233:80\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://136.25.2.43:56726\n",
      "Success! Added to List.\n",
      "Trying Proxy: https://69.195.157.162:8100\n",
      "Success! Added to List.\n"
     ]
    }
   ],
   "source": [
    "url_testing = 'http://www.foo.com/' #URL we'll use to check if a proxy works. Put your desired url to scrape here for best results.\n",
    "timeout_filtering = 4 #seconds, the timer we'll set on checking url_testing with proxies\n",
    "\n",
    "### We will use the 'proxyscrape' module for scraping proxies. Documentation here: https://pypi.org/project/proxyscrape/ ###\n",
    "\n",
    "#Initialize collector for proxies. RUN THIS ONCE.\n",
    "collector = create_collector('my-collector', 'https')\n",
    "\n",
    "#Retrieve a collector if already initialized\n",
    "# collector = get_collector('my-collector')\n",
    "\n",
    "#Outputs a list of working proxies and their corresponding geolocation (country)\n",
    "good_proxies, good_proxies_country  = scrape_proxies(collector, timeout_filtering, url_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display collected proxies in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Proxy</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.135.90.78:17567</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>166.70.207.2:8118</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>107.178.4.215:35330</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>104.45.188.43:3128</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>50.192.195.69:52018</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>54.214.52.181:80</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>24.172.34.114:49920</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>104.236.48.178:8080</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>198.204.253.114:3128</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>50.233.228.147:8080</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>144.217.101.245:3129</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>34.105.59.26:80</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>162.17.252.5:43764</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>160.2.38.41:8080</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>192.3.31.67:9998</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>173.82.177.226:5836</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>209.190.32.28:3128</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>198.55.103.233:80</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>136.25.2.43:56726</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>69.195.157.162:8100</td>\n",
       "      <td>united states</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Proxy        Country\n",
       "0      3.135.90.78:17567  united states\n",
       "1      166.70.207.2:8118  united states\n",
       "2    107.178.4.215:35330  united states\n",
       "3     104.45.188.43:3128  united states\n",
       "4    50.192.195.69:52018  united states\n",
       "5       54.214.52.181:80  united states\n",
       "6    24.172.34.114:49920  united states\n",
       "7    104.236.48.178:8080  united states\n",
       "8   198.204.253.114:3128  united states\n",
       "9    50.233.228.147:8080  united states\n",
       "10  144.217.101.245:3129         canada\n",
       "11       34.105.59.26:80  united states\n",
       "12    162.17.252.5:43764  united states\n",
       "13      160.2.38.41:8080  united states\n",
       "14      192.3.31.67:9998  united states\n",
       "15   173.82.177.226:5836  united states\n",
       "16    209.190.32.28:3128  united states\n",
       "17     198.55.103.233:80  united states\n",
       "18     136.25.2.43:56726  united states\n",
       "19   69.195.157.162:8100  united states"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip(good_proxies, good_proxies_country)), \n",
    "               columns =['Proxy', 'Country']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust downloading Function with Requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_download(url, current_proxy, good_proxies, timeout_req):\n",
    "    \n",
    "    #Put current/preferred proxy on top of list for iteration\n",
    "    if current_proxy in good_proxies:\n",
    "        good_proxies.insert(0, good_proxies.pop(good_proxies.index(current_proxy)))\n",
    "    else:\n",
    "        good_proxies.insert(0,current_proxy) \n",
    "        \n",
    "    for proxy in good_proxies:\n",
    "        \n",
    "        print(\"===================== DOWNLOADING =====================\")\n",
    "        \n",
    "        try:\n",
    "            if proxy == current_proxy:\n",
    "                print(\"Current Proxy: \" + proxy)\n",
    "            else:\n",
    "                print(\"Trying proxy: \" + proxy)\n",
    "                \n",
    "            #download webpage\n",
    "            req = requests.get(url, proxies={\"https\": \"https://\" + current_proxy}, timeout=timeout_req)\n",
    "            req.raise_for_status() #Throw an error if the timer runs out or there's a failure in downloading\n",
    "            \n",
    "            #Proxy works!\n",
    "            if proxy == current_proxy:\n",
    "                print(\"Still works: \" + proxy)\n",
    "            else:\n",
    "                print(\"Found new working proxy: \" + proxy)\n",
    "                \n",
    "            checker = True\n",
    "            current_proxy = proxy\n",
    "            break\n",
    "        except:\n",
    "            checker = False\n",
    "            print(\"Connection error. Trying next proxy.\")\n",
    "            continue \n",
    "            \n",
    "    if checker == False:\n",
    "        raise Exception(\"No working proxies!\") #Error thrown if none of the proxies are working\n",
    "    else:\n",
    "        print(\"Final proxy: \" + current_proxy)\n",
    "        print(\"Download Successful!\")\n",
    "        \n",
    "    return current_proxy #return new working proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Requests downloading example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== DOWNLOADING =====================\n",
      "Current Proxy: 198.55.103.233:80\n",
      "Still works: 198.55.103.233:80\n",
      "Final proxy: 198.55.103.233:80\n",
      "Download Successful!\n"
     ]
    }
   ],
   "source": [
    "#We can start our download with an initial/preferred proxy\n",
    "preferred_proxy = random.choice(good_proxies) #Just picking a random proxy from list\n",
    "\n",
    "url = 'http://www.foo.com/' #url you want to download webpages from\n",
    "timeout_req = 30 #seconds, the timer set on checking if you can download the pages with a proxy\n",
    "\n",
    "#Instead of requests.get(url), you will run this:\n",
    "preferred_proxy = robust_download(url, preferred_proxy, good_proxies, timeout_req) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
